return {
  -- {
  --   'ggml-org/llama.vim',
  --   init = function()
  --     -- vim.g.llama_config.endpoint = 'http://localhost:49152'
  --     vim.g.llama_config = {
  --       endpoint = 'http://localhost:49152/infill', -- Ensure this matches your llama.cpp server port
  --       show_info = 1, --status line
  --       max_line_suffix = 50,
  --       n_prefix = 200,
  --       n_predict = 256,

  --       -- auto_fim = true, -- Enable Fill-In-the-Middle completions
  --       -- show_info = true, -- Display performance stats
  --       -- keymap_toggle = '<C-F>', -- Toggle suggestion manually
  --       -- keymap_accept = '<Tab>', -- Accept full suggestion
  --       -- keymap_accept_line = '<S-Tab>', -- Accept first line of suggestion
  --     }
  --     vim.g.cache_data = {}
  --   end,
  -- },
}
